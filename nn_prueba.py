# -*- coding: utf-8 -*-
"""NN-prueba.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dw7D6rMvRy_81vaZ9cRWzJCtcezGvbKD
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras

from google.colab import drive
drive.mount('/content/drive')

#Cargar datos de entrenamiento y prueba

train_df = pd.read_excel('/content/drive/MyDrive/train_data.xlsx', names=['text', 'label'])
test_df = pd.read_excel('/content/drive/MyDrive/test_data.xlsx', names=['text', 'label'])


from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
training_labels = label_encoder.fit_transform(train_df['label'])
testing_labels = label_encoder.transform(test_df['label'])

print(train_df.head())
print(test_df.head())

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords = stopwords.words('english')
nltk.download('punkt')

def preprocess_text(text):
    tokens = nltk.word_tokenize(text)# Tokenizar el texto
    tokens = [token for token in tokens if token.lower() not in stopwords]
    tokens = [token.lower() for token in tokens]
    text = " ".join(tokens)
    return text

train_df['text'] = train_df['text'].apply(preprocess_text)
test_df['text'] = test_df['text'].apply(preprocess_text)

from tensorflow.keras.preprocessing.text import Tokenizer

vocab_size = 10000
tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')
tokenizer.fit_on_texts(train_df['text'])

training_sequences = tokenizer.texts_to_sequences(train_df['text'])
testing_sequences = tokenizer.texts_to_sequences(test_df['text'])

max_length = 100
training_padded = keras.preprocessing.sequence.pad_sequences(training_sequences, maxlen=max_length, truncating='post')
testing_padded = keras.preprocessing.sequence.pad_sequences(testing_sequences, maxlen=max_length, truncating='post')

training_labels = np.array(train_df['label'])
testing_labels = np.array(test_df['label'])

embedding_dim = 16

model = keras.Sequential([
    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    keras.layers.Conv1D(128, 5, activation='relu'),
    keras.layers.GlobalMaxPooling1D(),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(3, activation='softmax')
])

num_epochs = 10

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels))

_, test_acc = model.evaluate(testing_padded, testing_labels)

print('Test accuracy:', test_acc)

new_texts = ['This day is great!', 'This night is terrible']
new_sequences = tokenizer.texts_to_sequences(new_texts)
new_padded = keras.preprocessing.sequence.pad_sequences(new_sequences, maxlen=max_length, truncating='post')

predictions = model.predict(new_padded)

for i, text in enumerate(new_texts):
    print(text, predictions[i])

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

predictions = model.predict(testing_padded)

predicted_labels = np.argmax(predictions, axis=1)

cm = confusion_matrix(testing_labels, predicted_labels)

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

from sklearn.metrics import classification_report

predictions = model.predict(testing_padded)
predicted_labels = np.argmax(predictions, axis=1)
print(classification_report(testing_labels, predicted_labels))

from sklearn.metrics import precision_score
import seaborn as sns
import matplotlib.pyplot as plt

class_names = np.unique(train_df['label'])

precision = precision_score(testing_labels, predicted_labels, average=None)


sns.heatmap([precision], cmap='Blues', annot=True, cbar=False, fmt='.2g')
plt.xlabel('Clases')
plt.ylabel('Precisi√≥n')
plt.xticks(np.arange(len(class_names)) + 0.5, class_names, rotation=90)
plt.show()